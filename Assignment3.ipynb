{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 -  Clean and prepare your data:\n",
    "# The data in this exercise have been simulated to mimic real, dirty data.\n",
    "# Please clean the data with whatever method(s) you believe to be best/most suitable.\n",
    "# You may create new features. However, you may not add or supplement with external data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, precision_score, recall_score, roc_auc_score, roc_curve, auc\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from scipy.stats import chi2_contingency\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/sooriya/Documents/MLAlgorithms/data_project3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing duplicate rows if exists\n",
    "\n",
    "\n",
    "def duplicate_rows_cleaning(dataframe):\n",
    "    if dataframe.duplicated().sum() > 0:\n",
    "        dataframe = dataframe.drop_duplicates()\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "dataframe = df\n",
    "print(\"The number of instances after removing duplicate rows includes:{0}\".format(\n",
    "    duplicate_rows_cleaning(dataframe).shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the columns where\n",
    "# 1.if the one of the value in a column counts for more than or equal to 95%.\n",
    "\n",
    "\n",
    "def remove_columns(df):\n",
    "    col_to_drop = []\n",
    "    for col in df.columns:\n",
    "        most_common_pct = df[col].value_counts(normalize=True).iloc[0]\n",
    "        if most_common_pct >= 0.95:\n",
    "            col_to_drop.append(col)\n",
    "    print(col_to_drop)\n",
    "    df = df.drop(columns=col_to_drop)\n",
    "    return df\n",
    "\n",
    "\n",
    "df = remove_columns(df)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning the categorical columns.\n",
    "contingency_table = pd.crosstab(df['x31'], df['x93'])\n",
    "chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "if p < 0.05:\n",
    "    df = df.drop(columns=['x93'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# useof count encoding to replace categorical values to numerical ones.\n",
    "\n",
    "\n",
    "def clean_categorical_col(df):\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            col_dic = df[col].value_counts().to_dict()\n",
    "            df[col] = df[col].map(col_dic)\n",
    "    return df\n",
    "\n",
    "\n",
    "df = clean_categorical_col(df)\n",
    "y_data = df['y']\n",
    "X_data = df.drop(columns=['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use of simple imputer mean to fill the null values.\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_imputed = imputer.fit_transform(X_data)\n",
    "X_data = pd.DataFrame(X_imputed, columns=X_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identifying the correlated predictors and removing one of them to avoid redundant data.\n",
    "corr_matrix = X_data.corr(method='pearson', min_periods=1)\n",
    "col_to_drop = set()\n",
    "threshold_value = 0.7\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if abs(corr_matrix.iloc[i, j]) >= threshold_value:\n",
    "            # Get the name of one of the highly correlated columns\n",
    "            colname = corr_matrix.columns[i]\n",
    "            col_to_drop.add(colname)  # %%\n",
    "X_data = X_data.drop(columns=col_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using undersampling to nulify class imbalance\n",
    "under_sampler = RandomUnderSampler(random_state=50)\n",
    "X_data, y_data = under_sampler.fit_resample(X_data, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_data.shape)\n",
    "print(y_data.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardizing the values\n",
    "scaler = StandardScaler()\n",
    "X_data = scaler.fit_transform(X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_data, y_data, test_size=0.2, random_state=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-  Build your models:\n",
    "# For this exercise, you are required to build five models. The first model must be a logistic regression.\n",
    "# Then other 4 is the combination of bagging and boosting algorithms. Mimic the similar techniques we did during the class March 20th.\n",
    "# For each model, find the top 3 most important features. Shap value has to be calculated for one of your model and explain it in briefly.\n",
    "\n",
    "# 3- Evaluate your model:\n",
    "# For each model calculate accuracy, precision, recall and AUC score. Graph AUC score as well.\n",
    "# Create a table to make comparison for all models and decide which model is the best. State your reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression Model\n",
    "Model1 = LogisticRegression()\n",
    "Model1.fit(X_train, y_train)\n",
    "scores = cross_val_score(Model1, X_train, y_train, cv=5, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pred_val1 = Model1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy1 = accuracy_score(y_test, Pred_val1)\n",
    "print(\n",
    "    \"The accuracy score of Logistic regression model is:{0}\".format(accuracy1))\n",
    "class_report = classification_report(y_test, Pred_val1)\n",
    "print(\"The classification report of Logistic regression model is:\")\n",
    "print(class_report)\n",
    "precision = precision_score(y_test, Pred_val1)\n",
    "print(\n",
    "    \"The precision score of Logistic regression model is:{0}\".format(precision))\n",
    "recall = recall_score(y_test, Pred_val1)\n",
    "print(\"The recall score of Logistic regression model is:{0}\".format(recall))\n",
    "y_pred_probs = Model1.predict_proba(X_test)[:, 1]\n",
    "auc_score = roc_auc_score(y_test, y_pred_probs)\n",
    "print(\"the AUC Score is : {0}\".format(auc_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = abs(Model1.coef_[0])\n",
    "feature_series = pd.Series(feature_importance)\n",
    "sorted_features = feature_series.sort_values(ascending=False)\n",
    "top_three_features = sorted_features[:3]\n",
    "print(\"The top 3 features of Logistic regression model are : \")\n",
    "print(top_three_features.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a function to graph the roc curve for different models using roc score.\n",
    "\n",
    "\n",
    "def roccurve(roc_auc):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2,\n",
    "             label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(alpha=0.25)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_probs)\n",
    "roc_auc1 = auc(fpr, tpr)\n",
    "print(\"The ROC Curve for Linear regression is as below:\")\n",
    "print(roccurve(roc_auc1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shap values graph for the linear regression model\n",
    "explainer = shap.LinearExplainer(Model1, X_train)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "shap.summary_plot(shap_values, X_test)\n",
    "# The top features contribute more to the output variation than the bottom ones.\n",
    "# Here, x6 appears to be the most impactful feature, followed by x37, x30, etc.\n",
    "# This means that high values of x6 lead to higher predictions by the model,\n",
    "# while lower values of x6 tend to lower the model's predictions.\n",
    "# x6 has a wide spread, meaning its impact on the model's output varies a lot across different observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest Model\n",
    "forest = RandomForestClassifier()\n",
    "forest.fit(X_train, y_train)\n",
    "scores = cross_val_score(forest, X_train, y_train, cv=5, scoring='accuracy')\n",
    "print(\"the Accuracy score for the Trained data is  :{0}\".format(\n",
    "    np.mean(scores)))\n",
    "Pred_val2 = forest.predict(X_test)\n",
    "accuracy2 = accuracy_score(y_test, Pred_val2)\n",
    "print(\n",
    "    \"The accuracy score of Random Forest model is:{0}\".format(accuracy2))\n",
    "class_report = classification_report(y_test, Pred_val2)\n",
    "print(\"The classification report of Random Forest model is:\")\n",
    "print(class_report)\n",
    "precision = precision_score(y_test, Pred_val2)\n",
    "print(\n",
    "    \"The precision score of Random Forest model is:{0}\".format(precision))\n",
    "recall = recall_score(y_test, Pred_val2)\n",
    "print(\"The recall score of Random Forest model is:{0}\".format(recall))\n",
    "y_pred_probs = forest.predict_proba(X_test)[:, 1]\n",
    "auc_score = roc_auc_score(y_test, y_pred_probs)\n",
    "print(\"the AUC Score is : {0}\".format(auc_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_probs)\n",
    "roc_auc2 = auc(fpr, tpr)\n",
    "print(\"The ROC Curve for Random Forest model is as below:\")\n",
    "print(roccurve(roc_auc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = forest.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "print(\"Top three features in Random Forest model is:\")\n",
    "for i in indices[:3]:\n",
    "    print(f\"x{i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adaboost classifier\n",
    "ada_clf = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
    "ada_clf.fit(X_train, y_train)\n",
    "scores = cross_val_score(ada_clf, X_train, y_train, cv=5, scoring='accuracy')\n",
    "print(\"the Accuracy score for the Trained data is  :{0}\".format(\n",
    "    np.mean(scores)))\n",
    "Pred_val3 = ada_clf.predict(X_test)\n",
    "accuracy3 = accuracy_score(y_test, Pred_val3)\n",
    "print(\n",
    "    \"The accuracy score of Adaboost model is:{0}\".format(accuracy3))\n",
    "class_report = classification_report(y_test, Pred_val3)\n",
    "print(\"The classification report of Adaboost model is:\")\n",
    "print(class_report)\n",
    "precision = precision_score(y_test, Pred_val3)\n",
    "print(\n",
    "    \"The precision score of Adaboost model is:{0}\".format(precision))\n",
    "recall = recall_score(y_test, Pred_val3)\n",
    "print(\"The recall score of Adaboost model is:{0}\".format(recall))\n",
    "y_pred_probs = ada_clf.predict_proba(X_test)[:, 1]\n",
    "auc_score = roc_auc_score(y_test, y_pred_probs)\n",
    "print(\"the AUC Score is : {0}\".format(auc_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_probs)\n",
    "roc_auc3 = auc(fpr, tpr)\n",
    "print(\"The ROC Curve for Adaboost model is as below:\")\n",
    "print(roccurve(roc_auc3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = ada_clf.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "print(\"Top three features in Adaboost model is:\")\n",
    "for i in indices[:3]:\n",
    "    print(f\"x{i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# catboost\n",
    "catboost_model = CatBoostClassifier(\n",
    "    iterations=100, learning_rate=0.1, depth=3, verbose=0)\n",
    "catboost_model.fit(X_train, y_train)\n",
    "scores = cross_val_score(catboost_model, X_train,\n",
    "                         y_train, cv=5, scoring='accuracy')\n",
    "print(\"the Accuracy score for the Trained data is  :{0}\".format(\n",
    "    np.mean(scores)))\n",
    "Pred_val4 = catboost_model.predict(X_test)\n",
    "accuracy4 = accuracy_score(y_test, Pred_val4)\n",
    "print(\n",
    "    \"The accuracy score of catboost model is:{0}\".format(accuracy4))\n",
    "class_report = classification_report(y_test, Pred_val4)\n",
    "print(\"The classification report of catboost model is:\")\n",
    "print(class_report)\n",
    "precision = precision_score(y_test, Pred_val4)\n",
    "print(\n",
    "    \"The precision score of catboost model is:{0}\".format(precision))\n",
    "recall = recall_score(y_test, Pred_val4)\n",
    "print(\"The recall score of catboost model is:{0}\".format(recall))\n",
    "y_pred_probs = catboost_model.predict_proba(X_test)[:, 1]\n",
    "auc_score = roc_auc_score(y_test, y_pred_probs)\n",
    "print(\"the AUC Score is : {0}\".format(auc_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_probs)\n",
    "roc_auc4 = auc(fpr, tpr)\n",
    "print(\"The ROC Curve for catboost is as below:\")\n",
    "print(roccurve(roc_auc4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = catboost_model.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "print(\"Top three features in catboost model is:\")\n",
    "for i in indices[:3]:\n",
    "    print(f\"x{i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "xgboost_model = xgb.XGBClassifier(\n",
    "    n_estimators=100, learning_rate=0.1, max_depth=3, use_label_encoder=False, eval_metric='logloss')\n",
    "xgboost_model.fit(X_train, y_train)\n",
    "scores = cross_val_score(xgboost_model, X_train,\n",
    "                         y_train, cv=5, scoring='accuracy')\n",
    "print(\"the Accuracy score for the Trained data is  :{0}\".format(\n",
    "    np.mean(scores)))\n",
    "Pred_val5 = xgboost_model.predict(X_test)\n",
    "accuracy5 = accuracy_score(y_test, Pred_val4)\n",
    "print(\n",
    "    \"The accuracy score of XGBoost model is:{0}\".format(accuracy5))\n",
    "class_report = classification_report(y_test, Pred_val5)\n",
    "print(\"The classification report of XGBoost model is:\")\n",
    "print(class_report)\n",
    "precision = precision_score(y_test, Pred_val5)\n",
    "print(\n",
    "    \"The precision score of XGBoost model is:{0}\".format(precision))\n",
    "recall = recall_score(y_test, Pred_val5)\n",
    "print(\"The recall score of XGBoost model is:{0}\".format(recall))\n",
    "y_pred_probs = xgboost_model.predict_proba(X_test)[:, 1]\n",
    "auc_score = roc_auc_score(y_test, y_pred_probs)\n",
    "print(\"the AUC Score is : {0}\".format(auc_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_probs)\n",
    "roc_auc5 = auc(fpr, tpr)\n",
    "print(\"The ROC Curve for XGBoost model is as below:\")\n",
    "print(roccurve(roc_auc5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = xgboost_model.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "print(\"Top three features in XGBoost model is:\")\n",
    "for i in indices[:3]:\n",
    "    print(f\"x{i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table involving roc scores\n",
    "data = {\"roc_score\": [roc_auc1, roc_auc2, roc_auc3, roc_auc4, roc_auc5],\n",
    "        \"Accuracy\": [accuracy1, accuracy2, accuracy3, accuracy4, accuracy5]\n",
    "        }\n",
    "index = [\"Logistic Regression\", \"RandomForest\",\n",
    "         \"AdaBoost\", \"CatBoost\", \"XGBoost\"]\n",
    "Table = pd.DataFrame(data, index=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Table\n",
    "# From the table, XGBoost and CatBoost have the highest ROC scores (0.766633 and 0.765782, respectively),\n",
    "# indicating they are better at distinguishing between the positive and negative classes in your dataset compared to the other models.\n",
    "# AdaBoost and RandomForest show moderately high ROC scores, with Logistic Regression trailing slightly behind.\n",
    "# This suggests that ensemble methods (which RandomForest, AdaBoost, CatBoost, and XGBoost are) tend to perform better for\n",
    "# this particular task in terms of distinguishing between classes.\n",
    "\n",
    "# RandomForest shows the highest accuracy (0.689492), closely followed by XGBoost and CatBoost (both at 0.686477).\n",
    "# This suggests that, for correctly predicting the labels in your dataset,\n",
    "# RandomForest performs slightly better, with XGBoost and CatBoost being very close contenders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4- Summary of your findings / Conclusion\n",
    "\n",
    "\n",
    "# From the performance of all the models, we can see that the features like x6, x3 and x37\n",
    "# are the most important features for our classification.\n",
    "# with respect to the model, both XGBoost and Catboost have been best preforming model in terms of better classification than others."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
